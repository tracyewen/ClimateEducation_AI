{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29506dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vmuccion/switchdrive/NLP_transformers/ClimateEducation/cgpt_classification'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48acfa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, EarlyStoppingCallback, Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoModelForPreTraining\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ab940c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26026207",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Saeid\\Prj100\\SA_45_DSS_embrace\"\n",
    "train_file = \"seen1.xlsx\"\n",
    "test_file = \"unseen.xlsx\"\n",
    "train_path = os.path.join(path, train_file)\n",
    "test_path = os.path.join(path, test_file)\n",
    "cgpt_path = r\"C:\\Saeid\\Prj100\\SA_45_DSS_embrace\\models\\climateGPT2_ready_RL\\pytorch_model.bin\" # the path to bin file of cgpt((this is ckeygpt do we want cgpt or this?))\n",
    "output_path = r\"C:\\Saeid\\Prj100\\SA_45_DSS_embrace\\relevancy_cgpt_save\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1419cabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           DOI  \\\n",
      "0   10.1016/j.nedt.2021.105144   \n",
      "1   10.1007/s11422-021-10080-6   \n",
      "2     10.1016/j.pt.2010.06.009   \n",
      "3            10.1111/jnu.12326   \n",
      "4  10.1016/j.jsurg.2020.11.001   \n",
      "\n",
      "                                               Title  \\\n",
      "0  A mixed method study on global warming, climat...   \n",
      "1  Global challenges need attention now: educatin...   \n",
      "2  Soil-transmitted helminthiases: implications o...   \n",
      "3  Climate Change, Climate Justice, and Environme...   \n",
      "4  The Role of Preoperative Briefing and Postoper...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  AIM: This study aims to evaluate the knowledge...   \n",
      "1  This Editorial sets the stage for 18 papers on...   \n",
      "2  Soil-transmitted helminthiases (STHs) collecti...   \n",
      "3  PURPOSE: Climate change is an emerging challen...   \n",
      "4  OBJECTIVE: To study the impact of a new preope...   \n",
      "\n",
      "                            Source title  PubYear  \\\n",
      "0                  Nurse Education Today     2021   \n",
      "1  Cultural Studies of Science Education     2021   \n",
      "2                 Trends in Parasitology     2010   \n",
      "3         Journal of Nursing Scholarship     2017   \n",
      "4          Journal of Surgical Education     2020   \n",
      "\n",
      "                                             Authors  Times cited  \\\n",
      "0         Ergin, Emine; Altinel, Busra; Aktas, Emine            1   \n",
      "1             Tobin, Kenneth; Alexakos, Konstantinos            0   \n",
      "2  Weaver, Haylee J.; Hawdon, John M.; Hoberg, Er...           58   \n",
      "3             Nicholas, Patrice K.; Breakey, Suellen           44   \n",
      "4  Zhou, Nancy J; Kamil, Rebecca J; Hillel, Alexa...            0   \n",
      "\n",
      "                                      processed_text   topic_0   topic_1  ...  \\\n",
      "0  ['aim', 'studi', 'aim', 'evalu', 'knowledg', '...  0.000000  0.004469  ...   \n",
      "1  ['editori', 'set', 'stage', 'paper', 'theme', ...  0.034655  0.000000  ...   \n",
      "2  ['soil-transmit', 'helminthias', 'sths', 'coll...  0.022852  0.008059  ...   \n",
      "3  ['purpos', 'emerg', 'challeng', 'link', 'negat...  0.034234  0.000000  ...   \n",
      "4  ['object', 'studi', 'impact', 'new', 'preoper'...  0.000990  0.006498  ...   \n",
      "\n",
      "   Health  Energy  Water  COVID Adaptation  DRR  Mitigation  Impact  \\\n",
      "0     1.0     0.0    0.0    0.0        0.0  0.0         0.0       0   \n",
      "1     0.0     0.0    0.0    0.0        0.0  0.0         0.0       0   \n",
      "2     1.0     0.0    0.0    0.0        0.0  0.0         0.0       1   \n",
      "3     1.0     0.0    0.0    0.0        0.0  0.0         0.0       1   \n",
      "4     1.0     1.0    1.0    1.0        1.0  0.0         1.0       1   \n",
      "\n",
      "   Unnamed: 27  Unnamed: 28  \n",
      "0          NaN          NaN  \n",
      "1          NaN          NaN  \n",
      "2          NaN          NaN  \n",
      "3          NaN          NaN  \n",
      "4          NaN          NaN  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "                          DOI  \\\n",
      "0   10.1016/j.trd.2015.07.004   \n",
      "1         10.1093/poq/nfaa002   \n",
      "2         10.1787/22185514-en   \n",
      "3            10.7832/45-2-181   \n",
      "4  10.4278/0890-1171-19.2.103   \n",
      "\n",
      "                                               Title  \\\n",
      "0  Objective correlates and determinants of bicyc...   \n",
      "1  ADDRESSING GLOBAL WARMING DENIALISM THE EFFICA...   \n",
      "2  One Step Forward, Two Steps Backwards? Youth A...   \n",
      "3  True disciples, nature and leiturgia: Preserva...   \n",
      "4  Team awareness, problem drinking, and drinking...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  Objective Bicycle use for commuting is being e...   \n",
      "1  Despite the scientific consensus concerning th...   \n",
      "2  Young people across OECD countries enjoy unpre...   \n",
      "3  God created man in his image ( imago Dei ) and...   \n",
      "4  Purpose. (1) To determine the effectiveness of...   \n",
      "\n",
      "                                        Source title  PubYear  \\\n",
      "0  Transportation Research Part D Transport and E...   2015.0   \n",
      "1                           PUBLIC OPINION QUARTERLY   2020.0   \n",
      "2                     OECD Public Governance Reviews   2020.0   \n",
      "3                                        Missionalia   2017.0   \n",
      "4               AMERICAN JOURNAL OF HEALTH PROMOTION   2004.0   \n",
      "\n",
      "                                             Authors  Times cited  \\\n",
      "0  Cole-Hunter, T.; Donaire-Gonzalez, D.; Curto, ...           69   \n",
      "1  Rotman, Jeff D.; Weber, T. J.; Perkins, Andrew W.            2   \n",
      "2                                               OECD            0   \n",
      "3      Marumo, Phemelo Olifile; Van der Merwe, Sarel            0   \n",
      "4  Bennett, JB; Patterson, CR; Reynolds, GS; Wiit...           41   \n",
      "\n",
      "                                      processed_text   topic_0   topic_1  \\\n",
      "0  ['object', 'bicycl', 'use', 'commut', 'encoura...  0.008755  0.012644   \n",
      "1  ['despit', 'scientif', 'consensus', 'concern',...  0.028418  0.006693   \n",
      "2  ['young', 'peopl', 'oecd', 'countri', 'enjoy',...  0.030466  0.000000   \n",
      "3  ['god', 'creat', 'man', 'imag', 'imago', 'dei'...  0.017874  0.000000   \n",
      "4  ['purpos', 'determin', 'effect', 'classroom', ...  0.004793  0.008533   \n",
      "\n",
      "    topic_2   topic_3   topic_4  Topic                         Topic Label  \n",
      "0  0.006103  0.011916  0.005234      1  Adaptation/Livelihoods/Agriculture  \n",
      "1  0.017148  0.005094  0.014798      0             Sustainable development  \n",
      "2  0.000000  0.010823  0.002424      0             Sustainable development  \n",
      "3  0.001276  0.010707  0.000000      0             Sustainable development  \n",
      "4  0.023950  0.038815  0.002208      3                       HealthMedical  \n",
      "                                            Abstract  Topic\n",
      "0  AIM: This study aims to evaluate the knowledge...      3\n",
      "1  This Editorial sets the stage for 18 papers on...      0\n",
      "2  Soil-transmitted helminthiases (STHs) collecti...      3\n",
      "3  PURPOSE: Climate change is an emerging challen...      3\n",
      "4  OBJECTIVE: To study the impact of a new preope...      2\n",
      "                                               Abstract  Relevant\n",
      "1548  blair3sat is a student-run team based in Montg...         0\n",
      "1441  This study employs the multinomial endogenous ...         0\n",
      "308   Was genau ist ein Reallabor? Wie funktioniert ...         1\n",
      "430   The Education section of this issue features t...         0\n",
      "370   Evolution and whole systems ecology, thrive on...         1\n",
      "                                               Abstract  Relevant\n",
      "677   One of the most serious difficulties facing hu...         1\n",
      "1589  Widespread tropical deforestation and biodiver...         1\n",
      "984   This chapter seeks to evaluate the current sta...         0\n",
      "607   This timely Handbook brings innovative, free-t...         0\n",
      "1392  Purpose Although conformable devices are commo...         0\n"
     ]
    }
   ],
   "source": [
    "set_epoch = 100\n",
    "\n",
    "train_data = pd.read_excel(train_path)\n",
    "print(train_data.head())\n",
    "\n",
    "test_data = pd.read_excel(test_path)\n",
    "print(test_data.head())\n",
    "\n",
    "train_data.describe()\n",
    "\n",
    "test_data.describe()\n",
    "\n",
    "ready_to_train = train_data[[\"Abstract\", \"Topic\"]]\n",
    "print(ready_to_train.head())\n",
    "\n",
    "train, test = train_test_split(train_data[[\"Abstract\", \"Relevant\"]], test_size=0.33, random_state=42)\n",
    "\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "class relevancyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, tokenizer):\n",
    "        x = data[\"Abstract\"].tolist()\n",
    "        label = data[\"Relevant\"].tolist()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.x = x\n",
    "        self.label = label\n",
    "\n",
    "    # ---------------------------------------------#\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    # ---------------------------------------------#\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        encodings_dict = tokenizer(self.x[i],\n",
    "                                   truncation=True,\n",
    "                                   max_length=1500,\n",
    "                                   padding=\"max_length\")\n",
    "\n",
    "        input_ids = encodings_dict['input_ids']\n",
    "        attention_mask = encodings_dict['attention_mask']\n",
    "\n",
    "        return {'label':torch.tensor(self.label[i]),\n",
    "            'input_ids': torch.tensor(input_ids),\n",
    "            'attention_mask': torch.tensor(attention_mask)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "526556f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/14794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saeed\\AppData\\Local\\Temp/ipykernel_26908/578589286.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prcnt = round(softmax(logits).max().item()*100, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/14794\n",
      "2/14794\n",
      "3/14794\n",
      "4/14794\n",
      "5/14794\n",
      "6/14794\n",
      "7/14794\n",
      "8/14794\n",
      "9/14794\n",
      "10/14794\n",
      "11/14794\n",
      "12/14794\n",
      "13/14794\n",
      "14/14794\n",
      "15/14794\n",
      "16/14794\n",
      "17/14794\n",
      "18/14794\n",
      "19/14794\n",
      "20/14794\n",
      "21/14794\n",
      "22/14794\n",
      "23/14794\n",
      "24/14794\n",
      "25/14794\n",
      "26/14794\n",
      "27/14794\n",
      "28/14794\n",
      "29/14794\n",
      "30/14794\n",
      "31/14794\n",
      "32/14794\n",
      "33/14794\n",
      "34/14794\n",
      "35/14794\n",
      "36/14794\n",
      "37/14794\n",
      "38/14794\n",
      "39/14794\n",
      "40/14794\n",
      "41/14794\n",
      "42/14794\n",
      "43/14794\n",
      "44/14794\n",
      "45/14794\n",
      "46/14794\n",
      "47/14794\n",
      "48/14794\n",
      "49/14794\n",
      "50/14794\n",
      "51/14794\n",
      "52/14794\n",
      "53/14794\n",
      "54/14794\n",
      "55/14794\n",
      "56/14794\n",
      "57/14794\n",
      "58/14794\n",
      "59/14794\n",
      "60/14794\n",
      "61/14794\n",
      "62/14794\n",
      "63/14794\n",
      "64/14794\n",
      "65/14794\n",
      "66/14794\n",
      "67/14794\n",
      "68/14794\n",
      "69/14794\n",
      "70/14794\n",
      "71/14794\n",
      "72/14794\n",
      "73/14794\n",
      "74/14794\n",
      "75/14794\n",
      "76/14794\n",
      "77/14794\n",
      "78/14794\n",
      "79/14794\n",
      "80/14794\n",
      "81/14794\n",
      "82/14794\n",
      "83/14794\n",
      "84/14794\n",
      "85/14794\n",
      "86/14794\n",
      "87/14794\n",
      "88/14794\n",
      "89/14794\n",
      "90/14794\n",
      "91/14794\n",
      "92/14794\n",
      "93/14794\n",
      "94/14794\n",
      "95/14794\n",
      "96/14794\n",
      "97/14794\n",
      "98/14794\n",
      "99/14794\n",
      "100/14794\n",
      "101/14794\n",
      "102/14794\n",
      "103/14794\n",
      "104/14794\n",
      "105/14794\n",
      "106/14794\n",
      "107/14794\n",
      "108/14794\n",
      "109/14794\n",
      "110/14794\n",
      "111/14794\n",
      "112/14794\n",
      "113/14794\n",
      "114/14794\n",
      "115/14794\n",
      "116/14794\n",
      "117/14794\n",
      "118/14794\n",
      "119/14794\n",
      "120/14794\n",
      "121/14794\n",
      "122/14794\n",
      "123/14794\n",
      "124/14794\n",
      "125/14794\n",
      "126/14794\n",
      "127/14794\n",
      "128/14794\n",
      "129/14794\n",
      "130/14794\n",
      "131/14794\n",
      "132/14794\n",
      "133/14794\n",
      "134/14794\n",
      "135/14794\n",
      "136/14794\n",
      "137/14794\n",
      "138/14794\n",
      "139/14794\n",
      "140/14794\n",
      "141/14794\n",
      "142/14794\n",
      "143/14794\n",
      "144/14794\n",
      "145/14794\n",
      "146/14794\n",
      "147/14794\n",
      "148/14794\n",
      "149/14794\n",
      "150/14794\n",
      "151/14794\n",
      "152/14794\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26908/578589286.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Abstract\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m750\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"max_length\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_final\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;31m# print(logits)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# sigmoid = torch.nn.Sigmoid()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1197\u001b[1;33m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[0;32m   1198\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1199\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    745\u001b[0m                 )\n\u001b[0;32m    746\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m                 outputs = block(\n\u001b[0m\u001b[0;32m    748\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     ):\n\u001b[1;32m--> 289\u001b[1;33m         attn_outputs = self.attn(\n\u001b[0m\u001b[0;32m    290\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mpresent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0mattn_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36m_attn\u001b[1;34m(self, q, k, v, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn_dropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1376\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1832\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1833\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1834\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1835\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1836\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_model = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tt = GPT2Tokenizer.from_pretrained(base_model)\n",
    "tt.pad_token = tt.eos_token\n",
    "\n",
    "train_data_set = relevancyDataset(train, tokenizer)\n",
    "test_data_set = relevancyDataset(test, tokenizer)\n",
    "\n",
    "\n",
    "model_final = AutoModelForSequenceClassification.from_pretrained(output_path)\n",
    "model_final.config.pad_token_id = model_final.config.eos_token_id\n",
    "xd = []\n",
    "dx = []\n",
    "for k, i in enumerate(test_data[\"Abstract\"]):\n",
    "    print(str(k)+\"/\"+str(len(test_data[\"Abstract\"])))\n",
    "    inputs = tokenizer(i,truncation=True,max_length=750,padding=\"max_length\", return_tensors='pt')\n",
    "    logits = model_final(**inputs).logits\n",
    "\n",
    "    softmax= torch.nn.Softmax()\n",
    "\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    prcnt = round(softmax(logits).max().item()*100, 2)\n",
    "\n",
    "\n",
    "    xd.append(predicted_class_id)\n",
    "    dx.append(prcnt)\n",
    "req = pd.DataFrame({\"Abstract\":test_data[\"Abstract\"], \"Relevant\": xd, \"DOI\":test_data[\"DOI\"], \"Title\":test_data[\"Title\"], \"Source title\":test_data[\"Source title\"],\"PubYear\":test_data[\"PubYear\"],\"Authors\":test_data[\"Authors\"],\"Times cited\":test_data[\"Times cited\"],\"Probability\":dx})\n",
    "print(req.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe565ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = []\n",
    "for k, i in enumerate(train_data[\"Abstract\"]):\n",
    "    print(str(k)+\"/\"+str(len(train_data[\"Abstract\"])))\n",
    "    ff.append(100.0)\n",
    ")\n",
    "qer = pd.DataFrame({\"Abstract\":train_data[\"Abstract\"], \"Relevant\": train_data[\"Relevant\"], \"DOI\":train_data[\"DOI\"], \"Title\":train_data[\"Title\"], \"Source title\":train_data[\"Source title\"],\"PubYear\":train_data[\"PubYear\"],\"Authors\":train_data[\"Authors\"],\"Times cited\":train_data[\"Times cited\"],\"Probability\":ff})\n",
    "\n",
    "print(\"END!!\")\n",
    "print(qer.head())\n",
    "\n",
    "result = pd.concat([qer, req])\n",
    "print(result.head())\n",
    "rls = shuffle(result)\n",
    "print(rls.head())\n",
    "rls.to_excel(r\"C:\\Saeid\\Prj100\\SA_45_DSS_embrace\\relevancy_save\\output_relevant_unseen_seen_merge_shuffle.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e218b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
